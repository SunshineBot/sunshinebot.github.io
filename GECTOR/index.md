# Paper Note: GECToR – Grammatical Error Correction: Tag, Not Rewrite
 
## 整体思路

作者抛弃了自回归的文本生成方法，采用序列标注的方法来做GEC。序列标注的思路可能每一个做GEC的人都想过，因为看起来太契合了。不过作者的思路跟常规的src-trg的序列标注不一样，使用的是src-tag的方式，包含了非常多的人工标注知识在里面。作者借助grammarly的人力资源和知识储备，几乎将所有可能的修正模式进行了归纳和细分，最终整理成序列标注的标签。最终使用src+tag就可以得到trg，整个过程形成闭环。

## 具体思路

### 训练流程

-  基础模型：预训练语言模型（BERT/ALBERT/XLNET等） 
    - Stage I - pretrain：合成数据
    - Stage II - finetune：只有error数据（BEA）
    - Stage III - finetune：error数据+non-error数据（BEA）

### 作者思路

- token-level transformation and preprocessing
    - 基本变换：不变、删除、增加、替换（1+1+1167+3802个）
        - 增加和替换是依赖于token的tag，每个token和修改都有独立的tag
    - 语法变换：针对每种语法修改设计的规则（29个）
        - 大小写变换、名词单复数变换、动词形态变换等
    - 预处理：规则设计费时费力，实际预处理则会遇到最大序列标注最大的问题：一对多
        - 首先对语法修改进行拆分，例如go->goes to，拆分为[goes, to]两个修改，分别为动词形态变换、增加介词to
        - 语法规则是基于token level的，所以即使go->go to，也会拆成[go, to]，分别是不变、增加介词
        - 对于上述一对多的问题，作者的思路是*从中选一个*。。。
        - 本质上没有解决一对多的问题，通过选择一个的方式回避。但是通过规则的设置，解决部分规则明显的一对多问题。（例如拆分单词into->in to、忽略$KEEP标签实现1对2，等）
- 迭代纠错
    - 将纠错后的句子再次纠错。一般来说2次就可以达到很好的效果。4次及以上带来的提升幅度很小。(F0.5指标+2.5 on CoNLL2014)
- 实验和trick
    - 作者发现使用错误+正确的混合语料在常规finetune后再次finetune，能带来显著的提升（F0.5指标+3~6）。（Stage III很有效）
    - 在Stage I&II的前2个epoch，固定encoder参数，使用大于64的batch size，这两个方法能加速收敛并提升性能
    - 推断调整：给$KEEP标签的概率加上了confidence bias，提升$KEEP标签的概率，同时给错误标签设置了最小错误概率，以（通过提升精度的方式）提升F0.5。(F0.5指标+3~5)
    - 推断速度提升3~4倍（iter=1与beam=1对比）
    - 单模型：CoNLL2014=77.5/40.1/65.3，WI=79.2/53.9/72.4

## 总结

-  这篇论文提出了一个可行的使用序列标注模型解决GEC问题的方法。并通过设计大量标签作为tag作为学习目标的方法来合并不同词汇的同种修改。
- 作者没有完全解决序列标注模型中固有的一对多问题。但是作者通过语法错误标签的设计，合并了部分一对多问题。对于不能合并的多个标签，采用了选择一个标签的做法。最终将其变为一对一的序列标注问题。
    - 个人觉得迭代纠错的方法同样，通过对错误进行多次修改，同样能够缓解一对多的问题
- 在finetune后使用错误+正确的样本再次finetune能带来显著性能提升
- 模型的推断速度大幅提升。
- 在用了一些trick后模型性能超越SOTA。



Back to [Blog Index](../index)
